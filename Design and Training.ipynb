{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from collections import defaultdict, UserDict\n",
    "import mygrad as mg\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, string\n",
    "\n",
    "from noggin import create_plot\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipped_folder = \"glove.twitter.27B/\" # ENTER THE PATH TO THE UNZIPPED `glove.twitter.27B` HERE\n",
    "\n",
    "# use glove2word2vec to convert GloVe vectors in text format into the word2vec text format:\n",
    "if not Path('gensim_glove_vectors_200.txt').exists():\n",
    "    \n",
    "    # assumes you've downloaded and extracted the glove stuff\n",
    "    glove2word2vec(glove_input_file= unzipped_folder + \"glove.twitter.27B.200d.txt\", \n",
    "               word2vec_output_file=\"gensim_glove_vectors_200.txt\")\n",
    "\n",
    "# read the word2vec txt to a gensim model using KeyedVectors\n",
    "glove = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors_200.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.layers.conv import conv\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.activations.relu import relu\n",
    "from mygrad.nnet.layers import max_pool\n",
    "from mynn.activations.sigmoid import sigmoid\n",
    "from mynn.initializers.glorot_normal import glorot_normal\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes model layers and weights. \"\"\"\n",
    "        # <COGINST>\n",
    "        init_kwargs = {'gain': np.sqrt(2)}\n",
    "        self.conv1 = conv(200, 250, 2, stride = 1, weight_initializer = glorot_normal, weight_kwargs = init_kwargs)\n",
    "        self.dense1 = dense(250, 250, weight_initializer = glorot_normal, weight_kwargs = init_kwargs)\n",
    "        self.dense2 = dense(250,1, weight_initializer = glorot_normal, weight_kwargs = init_kwargs)\n",
    "        # </COGINST>\n",
    "    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\" Forward data through the network.\n",
    "        \n",
    "        This allows us to conveniently initialize a model `m` and then send data through it\n",
    "        to be classified by calling `m(x)`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(N, D, S)\n",
    "            The data to forward through the network.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(N, 1)\n",
    "            The model outputs.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        N = batch size\n",
    "        D = embedding size\n",
    "        S = sentence length\n",
    "        \"\"\"\n",
    "        # <COGINST>\n",
    "        # (N, D, S) with D = 200 and S = 77\n",
    "        x = self.conv1(x) # conv output shape (N, F, S') with F = 250 and S' = 75\n",
    "        x = relu(x)\n",
    "        x = max_pool(x, (x.shape[-1],), 1) # global pool output shape (N, F, S') with F = 250, S' = 1\n",
    "        x = x.reshape(x.shape[0], -1)  # (N, F, 1) -> (N, F)\n",
    "        x = self.dense1(x) # (N, F) @ (F, D1) = (N, D1)\n",
    "        x = relu(x) \n",
    "        x = self.dense2(x) # (N, D1) @ (D1, 1) = (N, 1)\n",
    "        x = sigmoid(x)\n",
    "        return x # output shape (N, 1)\n",
    "        # </COGINST>\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def parameters(self, load = None):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        return self.conv1.parameters + self.dense1.parameters + self.dense2.parameters # <COGLINE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\" Returns the word embedding for a given word, reshaping the word embedding array. \"\"\"\n",
    "    out = []\n",
    "    for word in text:\n",
    "        if word not in glove:\n",
    "            continue\n",
    "        else:\n",
    "            out.append(glove.get_vector(word))\n",
    "    while len(out) < 80:\n",
    "        out.append([0]*200)\n",
    "    return np.array(out).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "def strip_punc(text):\n",
    "    return punc_regex.sub('', text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ANSI', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test[0]==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = strip_punc(test[5][0]).split()\n",
    "sent2 = strip_punc(test[5][1]).split()\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = get_embedding(sent1)\n",
    "sent2 = get_embedding(sent2)\n",
    "overall = np.concatenate((sent1, sent2), axis = 0).reshape(-1, len(sent1), sent1.shape[1])\n",
    "overall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = strip_punc(test[5][0])\n",
    "sent2 = strip_punc(test[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = [sent1, sent2]\n",
    "check2 = batch_gen(check)\n",
    "check2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(text):\n",
    "    out = []\n",
    "    for each in text:\n",
    "        each=strip_punc(each).split()\n",
    "        out.append(get_embedding(each))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum('http'in item for item in (test.values)[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(item.split(' ')) for item in (test.values)[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, truth):\n",
    "    \"\"\" Calculates the accuracy of the predicted sentiments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pred: Union[numpy.ndarry, mygrad.Tensor]\n",
    "        The prediction scores of sentiments of the tweets (as a float from 0 to 1)\n",
    "    \n",
    "    truth: numpy.ndarry\n",
    "        The true tweet sentiment (0 or 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The accuracy of the predictions\n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    if isinstance(pred, mg.Tensor):\n",
    "        pred = pred.data\n",
    "    return np.mean(np.round(pred) == truth)\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y_truth):\n",
    "    \"\"\" Calculates the binary cross entropy loss for a given set of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: mg.Tensor, shape=\n",
    "        The Tensor of class scores output from the model\n",
    "    \n",
    "    y_truth: mg.Tensor, shape=\n",
    "        A constant Tensor or a NumPy array that contains the truth values for each prediction\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mg.Tensor, shape=()\n",
    "        A zero-dimensional tensor that is the loss\n",
    "    \"\"\"\n",
    "    return -mg.mean(y_truth * mg.log(y_pred + 1e-08) + (1 - y_truth) * mg.log(1 - y_pred + 1e-08)) # <COGLINE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_length = 4*len(test[0].values)//5\n",
    "values = test.values\n",
    "np.random.shuffle(values)\n",
    "polarity = values[:,0]\n",
    "polarity = polarity.astype('int16')\n",
    "polarity[polarity==4] = 1\n",
    "text = values[:,5]\n",
    "pol_train = polarity[0:slice_length]\n",
    "text_train = text[:slice_length]\n",
    "pol_test = polarity[slice_length:]\n",
    "text_test = text[slice_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = test.values\n",
    "vals[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optim = Adam(model.parameters, learning_rate = 1e-4)\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "for epoch_cnt in range(2):\n",
    "    idxs = np.arange(len(text_train))\n",
    "    np.random.shuffle(idxs)\n",
    "       \n",
    "    for batch_cnt in range(len(text_train)//batch_size):\n",
    "        # make slice object so indices can be referenced later\n",
    "        batch_indices = slice(batch_cnt * batch_size, (batch_cnt + 1) * batch_size)\n",
    "        batch = text_train[batch_indices]  # random batch of our training data\n",
    "        \n",
    "        # retrieve glove embeddings for batch\n",
    "        # <COGINST>\n",
    "        # initialize every value as small number which will be the placeholder for not found embeddings\n",
    "        # </COGINST>\n",
    "        embeddings = batch_gen(batch)\n",
    "        \n",
    "        # pass model through batch and perform gradient descent\n",
    "        # <COGINST>\n",
    "        pred = model(embeddings)\n",
    "        truth = pol_train[batch_indices]\n",
    "        \n",
    "        loss = binary_cross_entropy(pred[:,0], truth)\n",
    "        acc = accuracy(pred[:,0], truth)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        loss.null_gradients()\n",
    "        \n",
    "        # </COGINST>\n",
    "        \n",
    "        # pass loss and accuracy to noggin for plotting\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    # compute test statistics\n",
    "    idxs = np.arange(len(text_test))\n",
    "    for batch_cnt in range(0, len(text_test) // batch_size):\n",
    "        batch_indices = slice(batch_cnt * batch_size, (batch_cnt + 1) * batch_size)\n",
    "        batch = text_test[batch_indices]\n",
    "        \n",
    "        test_embeddings = batch_gen(batch)\n",
    "        \n",
    "        # perform forward pass and find accuracy but DO NOT backprop\n",
    "        # <COGINST>\n",
    "        pred = model(test_embeddings)\n",
    "        truth = pol_test[batch_indices]\n",
    "        acc = accuracy(pred[:,0], truth)\n",
    "        # </COGINST>\n",
    "\n",
    "        # log the test-accuracy in noggin\n",
    "        plotter.set_test_batch({\"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)\n",
    "   \n",
    "    # plot the epoch-level train/test statistics\n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_test[5], pol_test[5], model(batch_gen((text_test[5]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def save_model(model, path):\n",
    "        \"\"\"Path to .npz file where model parameters will be saved.\"\"\"\n",
    "        with open(path, \"wb\") as f:\n",
    "            np.savez(f, *(x.data for x in model.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'sentiment model.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
